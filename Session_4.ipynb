{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cvv-hoaBLFS",
        "colab_type": "text"
      },
      "source": [
        "#Compute Sigmoid function\n",
        "\n",
        "$sigmoid(x) = \\frac{1}{1 + e^{(-x)}}$\n",
        "\n",
        "Hint :\n",
        "\n",
        "  - import numpy as np\n",
        "  - exponent in numy can be called using 'np.exp'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c37mgKVN-juI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### START CODE HERE ### \n",
        "import numpy as np\n",
        "# Import some function\n",
        "### END CODE HERE ###\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "\n",
        "    ### START CODE HERE ### \n",
        "    s = 1 / (1 + np.exp(-x))\n",
        "    #s = None\n",
        "    ### END CODE HERE ###\n",
        "   \n",
        "\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aczikst0DFFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# call function sigmoid \n",
        "# set x = 0\n",
        "# print value of sigmoid\n",
        "\n",
        "\n",
        "### START CODE HERE ### \n",
        "\n",
        "### END CODE HERE ###\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piZkspQ7Pzx5",
        "colab_type": "text"
      },
      "source": [
        "### Expected Output :\n",
        "\n",
        "0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvnoGAWvCGRw",
        "colab_type": "text"
      },
      "source": [
        "# Compute Forward function\n",
        "\n",
        "\n",
        "\n",
        "### Equations :\n",
        "\n",
        "\n",
        "$$z = w^T x + b \\tag{1}$$\n",
        "$$\\hat{y}^{} = A^{} = sigmoid(z^{})\\tag{2}$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2JPDnGj7iL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(w, b, X):\n",
        "\n",
        "  ### START CODE HERE ###\n",
        "  # Note : w and x are vectors undergoing dot product\n",
        "  z = np.dot(w.T, X) + b \n",
        "  A = sigmoid(z)\n",
        "  #z = None \n",
        "  #A = None                                    \n",
        "  ### END CODE HERE ###\n",
        "                                  \n",
        "\n",
        "  return A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rHv_IJaQ-et",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "06455900-d4b9-495b-c075-c2f35884dc91"
      },
      "source": [
        "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
        "\n",
        "print(forward(w,b,X))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.99987661 0.99999386 0.00449627]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvwUHBQ7RZ1b",
        "colab_type": "text"
      },
      "source": [
        "### Expected Output :\n",
        "\n",
        "[[0.99987661 0.99999386 0.00449627]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZFScBEfIg7n",
        "colab_type": "text"
      },
      "source": [
        "# Compute Derivatives\n",
        "\n",
        "Here are the two formulas you will be using: \n",
        "\n",
        "Hint : use np.dot\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$  Hint : use np.sum\n",
        "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m ({A}-{Y})\\tag{8}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sG4K3GMAreg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def derivatives (A, X, Y):\n",
        "\n",
        "    m = X.shape[1]\n",
        "\n",
        "    ### START CODE HERE ### \n",
        "\n",
        "    dw = np.dot(X, (A - Y).T) / m\n",
        "    db = np.sum(A - Y) / m\n",
        "\n",
        "    ### END CODE HERE ### \n",
        "\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R08dEirAr4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "    \n",
        "    Tips:\n",
        "    You basically need to write down two steps and iterate through them:\n",
        "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
        "        2) Update the parameters using gradient descent rule for w and b.\n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    m = X.shape[1]\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        \n",
        "        # Cost and gradient calculation (≈ Call function computed above)\n",
        "        ### START CODE HERE ### \n",
        "        A = forward(w,b,X)\n",
        "        #A = None\n",
        "        cost = -np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A))) / m  # compute cost\n",
        "        grads = derivatives(A,X,Y)\n",
        "        #grads = None\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        # update rule (≈ 2 lines of code)\n",
        "        ### START CODE HERE ###\n",
        "        #w = None\n",
        "        #b = None\n",
        "        w = w - learning_rate*dw\n",
        "        b = b - learning_rate*db\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Record the costs\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eivKppevIN_V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "e5d62977-5f30-4163-8b94-8f653ecef37b"
      },
      "source": [
        "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
        "params, grads, costs = gradient_descent(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w = [[0.19033591]\n",
            " [0.12259159]]\n",
            "b = 1.9253598300845747\n",
            "dw = [[0.67752042]\n",
            " [1.41625495]]\n",
            "db = 0.21919450454067657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94atWhITLS5L",
        "colab_type": "text"
      },
      "source": [
        "**Expected Output**: \n",
        "\n",
        "<table style=\"width:40%\">\n",
        "<tr>\n",
        "    <td> **w** </td>\n",
        "    <td>[[ 0.19033591]\n",
        "[ 0.12259159]] </td>\n",
        "</tr>\n",
        "\n",
        "<tr>\n",
        "    <td> **b** </td>\n",
        "    <td> 1.92535983008 </td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td> **dw** </td>\n",
        "    <td> [[ 0.67752042] [ 1.41625495]] </td>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td> **db** </td>\n",
        "    <td> 0.219194504541 </td>\n",
        "</tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4j_t1P7cuX2",
        "colab_type": "text"
      },
      "source": [
        "## 3 - General Architecture of the learning algorithm ##\n",
        "\n",
        "It's time to design a simple algorithm to distinguish cat images from non-cat images.\n",
        "\n",
        "You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why **Logistic Regression is actually a very simple Neural Network!**\n",
        "\n",
        "<img src=\"LogReg_kiank.png\" style=\"width:650px;height:400px;\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8OoEPnsaaH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import Functions\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "from Utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "from lr_utils import load_dataset\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38bOhDpxag7t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "1755c480-a9a1-4de7-c386-907b7e21105b"
      },
      "source": [
        "# Loading the data (cat/non-cat)\n",
        "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9d1258567132>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set_x_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_x_orig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwV9DMPWaiAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example of a picture\n",
        "index = 25\n",
        "plt.imshow(train_set_x_orig[index])\n",
        "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\n",
        "\n",
        "m_train = train_set_x_orig.shape[0]     # 209\n",
        "m_test =  test_set_x_orig.shape[0]      # 50\n",
        "num_px = train_set_x_orig.shape[1]      # 64\n",
        "\n",
        "print (\"\\nNumber of training examples: m_train = \" + str(m_train))\n",
        "print (\"Number of testing examples: m_test = \" + str(m_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLaMntQRbYuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################### Data Pre-processing  ############################################\n",
        "\n",
        "\n",
        "# Reshape the training and test examples into single vectors of shape (num_px x num_px x 3, 1)\n",
        "\n",
        "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T\n",
        "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T\n",
        "\n",
        "\n",
        "# Let's standardize our dataset\n",
        "train_set_x = train_set_x_flatten/255.\n",
        "test_set_x = test_set_x_flatten/255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTuwC9Nrbclm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model using \"initialize_with_zeros\",\"optimize\" and \"predict\" functions from Utils\n",
        "\n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "\n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # initialize parameters with zeros\n",
        "    m = X_train.shape[0]\n",
        "    w, b = initialize_with_zeros(m)\n",
        "\n",
        "    # Gradient descent\n",
        "    parameters, grads, costs = gradient_descent(w,b,X_train,Y_train,num_iterations,learning_rate,print_cost)\n",
        "\n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "\n",
        "    # Predict test/train set examples\n",
        "    Y_prediction_test = predict(w,b,X_test)\n",
        "    Y_prediction_train = predict(w,b,X_train)\n",
        "\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test,\n",
        "         \"Y_prediction_train\": Y_prediction_train,\n",
        "         \"w\": w,\n",
        "         \"b\": b,\n",
        "         \"learning_rate\": learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "\n",
        "    return d\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFsXPaLtbfWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)\n",
        "\n",
        "\n",
        "# Plot learning curve (with costs)\n",
        "costs = np.squeeze(d['costs'])\n",
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}